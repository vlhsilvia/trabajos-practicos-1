{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/cabecera.png\" width=\"900\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo práctico 1: Análisis de sentimientos\n",
    "\n",
    "## Curso Procesamiento de Lenguaje Natural \n",
    "\n",
    "### Maestría en Tecnologías de la información\n",
    "\n",
    "\n",
    "\n",
    "**Trabajo práctico porpuesto por:** Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n",
    "**Desarrollado por:** _Silvia Vera Laceiras vlhsilvia@gmail.com_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este trabajo práctico tiene como objetivo el analizar y entender la importancia de cada paso que se debe llevar para la clasificación de textos. Los pasos que queremos analizar son:\n",
    "\n",
    "1. Normalización de datos\n",
    "3. *Stemming*\n",
    "4. Tokenización\n",
    "4. Extracción de características\n",
    "5. Clasificación\n",
    "\n",
    "Por esta razón, no vamos a dedicar tiempo a otras dos taeras fundamentales y que pueden consumir más de la mitad del tiempo de un proyecto de PLN: La obtención y limpieza de los datos. Por esta razón, vamos a mantenernos con el mismo conjunto de aprendizaje de las libretas del curso (TASS2015, tarea 1). \n",
    "\n",
    "La importancia de la libreta es experimentar cuales son las modificaciones que más impactan al resultado y cuales cambios no presentan modificaciones realmente. \n",
    "\n",
    "## 1. Cargando los datos y separando el conjunto de prueba y el de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta etapa ya la realizamos en forma estandar, vamos a cargar aquí todas los módulos a utilizar\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "from ast import literal_eval \n",
    "plt.rcParams['figure.figsize'] = (20, 12)\n",
    "\n",
    "# Carga el corpus del archivo pickle\n",
    "df_train = pd.read_pickle('datos/tass2015/general-tweets-train-dt.pkl')\n",
    "df_test = pd.read_pickle('datos/tass2015/general-tweets-test-dt.pkl')\n",
    "\n",
    "# Extrae los datos de entrenamiento y prueba para análisis de sentimiento\n",
    "x = dt_train['texto'].values\n",
    "x_test = dt_train['texto'].values\n",
    "y_polaridad = df_train['polaridad'].values\n",
    "\n",
    "# Separa los datos en conjunto de entrenamiento y validación\n",
    "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n",
    "    x, y_polaridad, test_size=0.1, random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Función de normalización de datos\n",
    "\n",
    "**Aquí pongo una función de normalización muy sencilla, pero la idea es que esta se modifique en función de las necesidades**\n",
    "\n",
    "Algunas ideas:\n",
    "\n",
    "1. ¿Hay ventajas en poner el texto en minúsculas o no?\n",
    "2. ¿Es mejor eliminar los usuarios y los *hashtags*, es mejor ponerlos con un nombre generico (USR o URL), o es mejor dejarlos como están?\n",
    "3. ¿Es mejor eliminar todos los signos de puntuación? ¿Mejor recuperar alguno?\n",
    "4. ¿Que pasa si los emoticones los cambiamos por las palabras `feliz` o `triste`?\n",
    "5. ¿Vale la pena eliminar las *palabras vacias*\n",
    "\n",
    "Escribe a continuación al menos una idea de normalización que consideres puede ser útil. Es muy importante que esto lo hagas *antes* de programarlo y probarlo. No importa si son o no útiles al final, lo importante en este caso es adquirir experiencia práctica.\n",
    "\n",
    "1. Podemos recuperar y clasificar dentro de los tweets por ejemplo la locaclizacion o a que continente pertenece \n",
    "2. Conservar la edad de los usuariosb de tweeter para clasificar por rango etario por ejemplo..\n",
    "\n",
    "Por último, vamos a agregar la opción de realizar el proceso de *steeming* o no, utilizando el método clásico propuesto para español por *Snowball*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las expresiones regulares precompiladas es mejor realizarlas fuera de la función por eficiencia\n",
    "remplaza_por_espacios_re = re.compile('[\\n/(){}\\[\\]\\|@,;\\.]')\n",
    "simbolos_a_eliminar_re = re.compile('[^\\d\\w #+_]')\n",
    "palabras_vacias = set(nltk.corpus.stopwords.words('spanish'))\n",
    "\n",
    "#TODO: Modifica la normalización de texto\n",
    "def normaliza_texto(texto, stemmer=None):\n",
    "    \"\"\"\n",
    "    Normaiza un documento\n",
    "    \n",
    "    :param texto: Una cadena de caracteres\n",
    "    :param steemer: Una objeto steemer, si es None, entonces no hace steeming del texto\n",
    "    \n",
    "    :return: Una cadena de caracteres con el texto modificado\n",
    "    \"\"\"\n",
    "    text = texto.lower()\n",
    "    \n",
    "    # AGREGA AQUI EL CODIG QUE SEA NECESARIO Y EN EL ORDEN NECESARIO\n",
    "\n",
    "    # Eliminación de simbolos\n",
    "    text = re.sub(remplaza_por_espacios_re, ' ', text)\n",
    "    text = re.sub(simbolos_a_eliminar_re, '', text)\n",
    "    \n",
    "    # Elimina palabras de paro\n",
    "    text = ' '.join([palabra for palabra in text.split() \n",
    "                     if palabra not in palabras_vacias])\n",
    "    \n",
    "    # Steeming\n",
    "    if stemmer is not None:\n",
    "        text = ' '.join([stemmer.stem(palabra) for palabra in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy importante, en cada paso, hacer una prueba para ver si el método está funcionando como lo diseñaste. En particular las expresiones regulares dan muchas sorpresas. Vamos a seleccionar un documento al azar y lo vamos a normalizar. Realiza esto varias veces para ver diferentes documentos y estar seguro el lo que programaste funcione correctamente.\n",
    "\n",
    "En su lugar puedes poner un ejemplo específico con los detalles que quieras revisar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = np.random.choice(x_train)\n",
    "doc_norm = normaliza_texto(doc)\n",
    "\n",
    "print(\"{}\\n\\n{}\".format(doc, doc_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora con *stemming*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SpanishStemmer()\n",
    "doc = np.random.choice(x_train)\n",
    "doc_norm = normaliza_texto(doc, stemmer)\n",
    "\n",
    "print(\"{}\\n\\n{}\".format(doc, doc_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder aplicar este método a todo el ndarray de nuestro corpora, hay que vectorizar la funcion (y así llamarla para todo elemento). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliza_texto_np = np.vectorize(normaliza_texto, excluded=['stemmer'])\n",
    "\n",
    "# Ejemplo\n",
    "normaliza_texto_np(x_train[:5], stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenización y extracción de características\n",
    "\n",
    "**Esta funcion la debes de completar, algunas cosas se codifican internamente**\n",
    "\n",
    "Para la tokenización y extracción de características vamos a utilizar los métodos que provee *sklearn*, los cuales los vamos a ajustar de acuerdo a parámetros que escojamos. \n",
    "\n",
    "Entre las desiciones que hay que tomar es:\n",
    "1. ¿Cual metodo de tokenización? La tokenización en los métodos incluidos en *sklearn* se definen por expresiones regulares. Por default, *sklearn* considera un token solo aquellos grupos de al menos dos letras, pero elimina todos los símbolos y los números. Esto en algunos casos puede no ser la mejor opción.\n",
    "\n",
    "2. Tipo de tokenizador. Vamos a mantener tres posibles tokenizadores:\n",
    "    a. BOW\n",
    "    b. Cuentas (BOW con frecuencia del token en el documento)\n",
    "    c. TF-IDF\n",
    "    \n",
    "3. Número mínimo de documentos en los que debe aparecer un token para ser considerado, y porcentaje máximo de documentos en los que un token aparezca para considerarlo con poder de discriminación.\n",
    "\n",
    "4. Número mínimo y máximo de $n$-gramas a considerar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracion_caracteristicas(x, tipo=\"BOW\", token_re=r'(?u)\\b\\w\\w+\\b', min_df=5, max_df=0.9, bigramas=False):\n",
    "    \"\"\"\n",
    "    Genera un objeto tipo CountVectorizer o TdfdfVectorizer\n",
    "    \n",
    "    :param x: Un ndarray con el texto de entrenamiento\n",
    "    :param tipo: Una cadea que puede ser \"BOW\", \"Count\" o \"TF-IDF\"\n",
    "    :param token_re: Una expresión regular que defina los tokens (por default r'(?u)\\b\\w\\w+\\b')\n",
    "    :param min_df: int, número mínimo de documentos donde debe aparecer el token (5 por default)\n",
    "    :param max_df: float 0.0 <= max_df <= 1.0, porcentaje máximo donde aparece un token significativo (0.9 default)\n",
    "    :param bigramas: bool, True si se incluyen unigramas y bigramas, False, solo unigramas\n",
    "    \n",
    "    :return un objeto tipo CountVectorizer\n",
    "    \n",
    "    \"\"\"\n",
    "    ngram_range = (1,2) if bigramas else (1,1)\n",
    "    \n",
    "    if tipo is \"BOW\":\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "            analyzer='word', binary=True, token_pattern=token_re, \n",
    "            min_df=min_df, max_df=max_df, ngram_range=ngram_range            \n",
    "        )\n",
    "    elif tipo is \"Count\":\n",
    "        vectorizer = None # <----- Desarrollar aqui\n",
    "    elif tipo is \"TF-IDF\":\n",
    "        vectorizer = None # <----- Desarrollar aqui\n",
    "    else:\n",
    "        raise NotImplementedError(\"El método {} no está contemplado\".format(tipo))\n",
    "    vectorizer.fit(x)\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probarlo vamos a extraer características de los primeros 50 ejemplos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_re = None # <-- Cambialo aqui para probar con otros me´todos de tokenización\n",
    "\n",
    "x_ejemplo = normaliza_texto_np(x_train[:50])\n",
    "if t_re is None:\n",
    "    vectorizer = extracion_caracteristicas(x_ejemplo, min_df=1, max_df=1.0)\n",
    "else:\n",
    "    vectorizer = extracion_caracteristicas(x_ejemplo, min_df=1, max_df=1.0, token_re=t_re)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora **contesta las siguiente preguntas** (Aquí mismo)\n",
    "\n",
    "1. ¿Cuantas palabras tiene el vocabulario con el método de tokenización por default?\n",
    "2. ¿Cuantas palabras tiene el vocabulario si onsideras como token cualquie serie de uno o más\n",
    "   signos diferentes a un simbolo de espacio en blanco?\n",
    "3. Si entrenas con el método de 'TF-IDF' ¿Cuales son los tres tokens con mayor valor?\n",
    "4. Responde a la misma pregunta, pero en el caso que incluyas bigramas\n",
    "5. ¿Cual es el token que más aparece en cada caso?\n",
    "\n",
    "Agrega aqui abajo el código necesario para responder a las preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega aqui todo el código necesario\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clasificación\n",
    "\n",
    "En la libreta del curso practicamos utilizando un clasificador logístico, el cual no ajustamos, sin embargo existen otros clasificadores, los cuales pueden tener un resultado. En particular nos interesa el método de máquinas de vectores de soporte (SVM por Support Vector Machines), las cuales se utilizan también en forma usual para la clasificación de texto. Para una explicación sobre como funcionan las máquina de vectores de soporte se puede consultar este [capítulo](https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf) que a mi me gusta mucho.\n",
    "\n",
    "Lo mejor de utilizar los métodos existentes en *sklearn* es que solamente se requiere entender la idea general del método y un poco de intuición sobre como funcionan los parámetros. En la [documentación de *sklearn*](http://scikit-learn.org/stable/modules/svm.html#svm) se encuentra una explicacion de las SVM para la clasificación con múltiples clases. De la documentación podemos concluir que existen dos algoritmos:\n",
    "\n",
    "1. Un algoritmo para la aproximación lineal (similar a la regrasión logística) con `LinearSVC`. Para este algoritmo hay dos parámetros a ajustar: la intensidad de la regulación `C` y la norma para la penalización `penalty`, la cual puede ser `l2` o `l1`(exactamente como en la regresión logística).\n",
    "\n",
    "2. Un algoritmo para la aproximación utilizando un kernel (el más utilizado es el gaussiano, por default). En este caso el único parámetro que vamos a ajustar es `C`.\n",
    "\n",
    "\n",
    "Ahora es necesario desarrollar una función que nos devuelva el tipo de clasificador que queremos, ya entrenado. **Desarrolla la función aquí abajo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_clasificador(x, y, tipo='logistica', C=1.0, penalty='l2'):\n",
    "    \"\"\"\n",
    "    Genera un clasificador segun el tipo que escojamos, y lo preentrenamos con x, y y\n",
    "    \n",
    "    :param x: ndarray de shape (n_ejemplos, n_caracteristicas) con los ejemplos de entrenamiento\n",
    "    :param y: ndarray de shape (n_ejemplos,) con las clases asignadas al conjunto de entrenamiento\n",
    "    :param tipo: tipo in ['logistica', 'SVClin, 'SVCgauss'] con el tipo de clasificador a utilizar\n",
    "    :param C: float, con la fuerza de la regularización\n",
    "    :param penalty: penalty in ['l2', 'l1'] la norma de la penalización, no aplica para SVC\n",
    "    \n",
    "    :return un objeto de una clase heredada de ClassifierMixin, con un clasificador de sklearn y sus \n",
    "            métodos estandar\n",
    "    \n",
    "    \"\"\"\n",
    "    if tipo is 'logistica':\n",
    "        clf = sklearn.linear_model.LogisticRegression(C=C, penalty=penalty)\n",
    "    elif tipo is 'SVClin':\n",
    "        clf = None # <---- COMPLETAR EL CÓDIGO\n",
    "    elif tipo is 'SVCgauss':\n",
    "        clf = None # <---- COMPLETAR EL CÓDIGO\n",
    "    else:\n",
    "        raise NotImplementedError(\"El clasificador {} no lo tenemos disponible todavía\".format(tipo))\n",
    "        \n",
    "    clf.fit(x, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último copiamos el código de reporte de resultados, aunque con una modificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporte(y_real, y_estimada, labels=None):\n",
    "    print(\"\\nPorcentaje de acierto: {}\".format(sklearn.metrics.accuracy_score(y_real, y_estimada)))\n",
    "    print(\"\\nPrecisión, recall y f1-score\")\n",
    "    print(sklearn.metrics.classification_report(y_real, y_estimada, target_names=labels))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Poniendo todo junto\n",
    "\n",
    "Ahora vamos a analizar cuales son los factores que más afectan en el análisis de sentimientos. Para eso vamos a poner todo junto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETROS SELECCIONADOS \n",
    "\n",
    "# Normalización\n",
    "stemmer = None \n",
    "\n",
    "# Tokenización\n",
    "token_re = r'(?u)\\b\\w\\w+\\b'\n",
    "\n",
    "# Extracción de características\n",
    "tipo_vec = \"BOW\"\n",
    "min_df = 5\n",
    "max_df = 0.9\n",
    "bigramas = False\n",
    "\n",
    "# Clasificación\n",
    "tipo_clf = 'logistica'\n",
    "C = 1.0\n",
    "penalty = 'l2'\n",
    "\n",
    "## EL PROCESO\n",
    "x_train_norm = normaliza_texto_np(x_train, stemmer) \n",
    "x_val_norm = normaliza_texto_np(x_val, stemmer)\n",
    "\n",
    "vectorizador = extracion_caracteristicas(\n",
    "    x_train_norm, tipo=tipo_vec, token_re=token_re, \n",
    "    min_df=min_df, max_df=max_df, bigramas=bigramas\n",
    ")\n",
    "x_train_vec = vectorizador.transform(x_train_norm)\n",
    "x_val_vec = vectorizador.transform(x_val_norm)\n",
    "\n",
    "clasificador = genera_clasificador(\n",
    "    x_train_vec, y_train, tipo=tipo_clf, C=C, penalty=penalty\n",
    ")\n",
    "y_est_train = clasificador.predict(x_train_vec)\n",
    "y_est_val = clasificador.predict(x_val_vec)\n",
    "\n",
    "print(\"Resultados con los datos de entrenamiento\")\n",
    "reporte(y_train, y_est_train)\n",
    "print(\"Resultados con los datos de entrenamiento\")\n",
    "reporte(y_val, y_est_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contesta las sigientes preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Manteniendo los parámetros de extracción de características y los del clasificador por default, ¿Cuales son los métodos de normalización, combinados con el tokenizador, que mejores resultados dan? Explica al menos unos 5 cambios que hayas probado y que influyeron mucho en el resultado final, ya sea para bien, ya sea para mal. Recuerda que las operaciones sobre la normalización las tienes que realizar directamente en la función correspondiente.\n",
    "    1. _Primer modificacion explicada_\n",
    "    2. _Segunda modificación explicada_\n",
    "    3. _Tercera modificación explicada_\n",
    "    4. _Cuarta modificación explicada_\n",
    "    5. _Quinta modificación explicada_\n",
    "    6. _Agrega cuantas consideres interesantes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Existe una diferencia notable entre utilizar BOW, Count y TF-IDF? ¿Cual es el mejor método en este problema particular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Cual es la mejor selección de min_df, max_df? ¿Tienen una influencia importante sobre los resultados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Completa la siguiente tabla\n",
    "\n",
    "| Vectorizador     | Clasificador  | Mejor C | Mejor penalty | *Accuracy* | F1-score |\n",
    "| ---------------- |:------------- | :------ | :-----------: | :--------- | :------- |\n",
    "| TF-IDF unigramas | Logístico     |         |               |            |          |\n",
    "| TF-IDF bigramas  | Logístico     |         |               |            |          |\n",
    "| TF-IDF unigramas | SVC lineal    |         |               |            |          |\n",
    "| TF-IDF bigramas  | SVC lineal    |         |               |            |          |\n",
    "| TF-IDF unigramas | SVC gauss     |         |   No aplica   |            |          |\n",
    "| TF-IDF bigramas  | SVC gauss     |         |   No aplica   |            |          |\n",
    "\n",
    "Si con tu normalización, el mejor método de extracción de características no es $TF-IDF$ tienes toda la libertad de cambiar el vectorizador de la tabla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por último escribe aqui un párrafo breve con tus conclusiones\n",
    "\n",
    "\n",
    "--Ingresar el texto aquí--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
